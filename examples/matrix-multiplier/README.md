# Matrix Multiplier: Computational Mathematics in Rust

A comprehensive demonstration of matrix multiplication algorithms showcasing Rust's computational capabilities from naive O(nÂ³) implementations to advanced optimization techniques.

## ğŸ§® Mathematical Progression

### Algorithm Complexity Hierarchy

1. **Naive O(nÂ³)**: Basic three-nested-loop implementation (REFERENT)
2. **Parallel Naive**: Rayon-based parallelization (BENCHMARK vs referent)
3. **Blocked/Tiled**: Cache-optimized memory access patterns
4. **Parallel Blocked**: Combined parallelization and cache optimization
5. **Strassen O(n^2.807)**: Divide-and-conquer recursive algorithm

### DSKY-Inspired Performance Framework

Following our established paradigm:
- **Referents (NOUNs)**: Sequential algorithms as mathematical baselines
- **Benchmarks (VERBs)**: Parallel/optimized implementations as transformations
- **Performance Space**: Navigate through [time, GFLOPS, speedup, efficiency] coordinates

## ğŸš€ Features

### Algorithm Implementations

- **Naive Sequential**: Pure three-loop implementation for baseline measurement
- **Parallel Naive**: Rayon-based row-wise parallelization
- **Cache-Blocked**: Tiled multiplication optimizing for memory hierarchy
- **Parallel Blocked**: Combined threading and cache optimization
- **Strassen Recursive**: Sub-cubic complexity divide-and-conquer

### Performance Analysis

- **GFLOPS Measurement**: Floating-point operations per second calculation
- **Scaling Analysis**: Performance across different matrix sizes
- **Memory Pattern Study**: Cache efficiency and access pattern optimization
- **Parallel Efficiency**: Thread utilization and speedup analysis

## ğŸ“Š Usage Examples

```bash
# Basic algorithm benchmark
matrix-multiplier benchmark 512 --iterations 5

# Scaling analysis across sizes  
matrix-multiplier scaling --start-size 64 --end-size 1024 --factor 2

# Optimization technique demonstration
matrix-multiplier techniques 256

# Memory access pattern analysis
matrix-multiplier memory 512
```

## ğŸ”¬ Mathematical Insights

### Computational Complexity

Matrix multiplication demonstrates fundamental algorithmic complexity concepts:

```
Naive:     O(nÂ³) - 2nÂ³ floating-point operations
Strassen:  O(n^2.807) - Asymptotically superior for large n
Blocked:   O(nÂ³) but with better cache performance
```

### Performance Metrics

**GFLOPS Calculation**:
```rust
GFLOPS = (2 * nÂ³) / (time_seconds * 10â¹)
```

Where `2 * nÂ³` represents the total floating-point operations (multiply + add for each element).

### Memory Hierarchy Optimization

**Cache-Blocked Algorithm**:
- Divides computation into blocks fitting in CPU cache
- Improves spatial and temporal locality
- Reduces memory bandwidth requirements

**Parallel Efficiency**:
```
Speedup = T_sequential / T_parallel
Efficiency = Speedup / Number_of_Threads
```

## ğŸ¯ Educational Objectives

This matrix multiplier demonstrates:

1. **Algorithm Complexity**: From O(nÂ³) to O(n^2.807) improvements
2. **Memory Hierarchy**: Cache-conscious programming techniques  
3. **Parallel Computing**: Thread-based performance scaling
4. **Performance Analysis**: GFLOPS measurement and efficiency calculation
5. **Mathematical Optimization**: Strassen's divide-and-conquer approach

## ğŸ§ª Benchmark Results Preview

Expected performance characteristics:

```
ğŸ“Š Algorithm        | Time    | GFLOPS | Speedup
ğŸ“Š Naive O(nÂ³)      | 2.450s  | 0.22   | 1.00x
ğŸ“Š Parallel Naive   | 0.680s  | 0.79   | 3.60x  
ğŸ“Š Blocked (64)     | 1.890s  | 0.29   | 1.30x
ğŸ“Š Parallel Blocked | 0.520s  | 1.04   | 4.71x
ğŸ“Š Strassen O(n^2.8)| 1.120s  | 0.48   | 2.19x
```

## ğŸ”— Connection to CLI Trilogy

This matrix multiplier extends your mathematical journey:

- **file-processor**: Simple abelian operations â†’ **Matrix**: Mathematical computation
- **log-analyzer**: Structured parsing â†’ **Matrix**: Algorithmic analysis  
- **concurrent-processor**: Non-abelian concurrency â†’ **Matrix**: Computational mathematics
- **matrix-multiplier**: Pure mathematical algorithms and optimization

Together, these demonstrate the full spectrum from systems programming to computational mathematics, all unified by the DSKY-inspired performance measurement framework.

## ğŸŒŸ Advanced Features

### Memory Access Analysis
- Different loop orderings (ijk, ikj, jik, etc.)
- Cache miss analysis and optimization
- Memory bandwidth utilization studies

### Numerical Stability
- Floating-point precision considerations
- Error accumulation analysis
- Verification against reference implementations

This serves as both a practical demonstration of matrix algorithms and a mathematical exploration of computational complexity in modern systems programming.
